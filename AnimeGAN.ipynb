{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4lKQjGQ_sj8",
        "outputId": "fe204690-9ac9-4fad-acaa-d60855a1d1e4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDwZUdTW_fcV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Anime GAN Code/losses\")\n",
        "sys.path.append(\"/content/drive/MyDrive/Anime GAN Code/model\")\n",
        "sys.path.append(\"/content/drive/MyDrive/Anime GAN Code\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxwE0Bt2-xnl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\n",
        "    BatchNormalization, Activation, Dense, Dropout, Flatten, Multiply, Add, Lambda, SpatialDropout2D, Reshape, GlobalMaxPooling2D, Layer, UpSampling2D\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "from tensorflow.keras.optimizers.schedules import (\n",
        "    ExponentialDecay,\n",
        "    PolynomialDecay,\n",
        "    PiecewiseConstantDecay,\n",
        "    CosineDecay,\n",
        "    CosineDecayRestarts,\n",
        "    InverseTimeDecay,\n",
        "    LearningRateSchedule\n",
        ")\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "import support_fun\n",
        "\n",
        "from model.encoder import build_encoder\n",
        "from model.main_discriminator import build_main_discriminator\n",
        "from model.main_gan import build_main_gan\n",
        "from model.main_generator import build_main_generator\n",
        "from model.support_discriminator import build_support_discriminator\n",
        "from model.support_gan import build_support_gan\n",
        "from model.support_generator import build_support_generator\n",
        "\n",
        "from losses.color import color_loss\n",
        "from losses.content import content_loss\n",
        "from losses.dm import dm_loss\n",
        "from losses.ds import ds_loss\n",
        "from losses.gray_style import gray_style_loss\n",
        "from losses.m_adv import m_adv_loss\n",
        "from losses.per_pixel import per_pixel_loss\n",
        "from losses.perception import perception_loss\n",
        "from losses.region_smoothing import region_smoothing_loss\n",
        "from losses.s_adv import s_adv_loss\n",
        "from losses.tv import total_loss\n",
        "from losses.vgg19 import build_vgg19\n",
        "\n",
        "from support_fun import fine_grained_revision, guided_filter_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "uy6kOLKB-xno",
        "outputId": "e9cdb353-203f-415c-ba77-489f60f5eaaa"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "file_id = ''\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'anime_file.zip', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is_QmOk9-xnp"
      },
      "outputs": [],
      "source": [
        "!unzip -q anime_file.zip -d anime_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "lHrdR7lC-xnp",
        "outputId": "e8d6205a-de8e-44de-99f5-13b351726849"
      },
      "outputs": [],
      "source": [
        "file_id = ''\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'landscape_file.zip', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5bhfhHO-xnp"
      },
      "outputs": [],
      "source": [
        "!unzip -q landscape_file.zip -d landscape_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "2CFi32Qb-xnp",
        "outputId": "aa89101b-ff6c-4ff0-d790-063729875ed3"
      },
      "outputs": [],
      "source": [
        "file_id = ''\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', 'blurred_file.zip', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zF6dMhG-xnp"
      },
      "outputs": [],
      "source": [
        "!unzip -q blurred_file.zip -d blurred_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DpazGaO-xnq"
      },
      "outputs": [],
      "source": [
        "def create_dataset(data_dir, batch_size=8):\n",
        "    \"\"\"Create train and validation datasets with a split.\"\"\"\n",
        "    data_dir = pathlib.Path(data_dir)\n",
        "    all_image_paths = list(data_dir.glob('*.jpg'))\n",
        "    all_image_paths = [str(path) for path in all_image_paths]\n",
        "\n",
        "    # Shuffle the paths\n",
        "    total_images = len(all_image_paths)\n",
        "    tf.random.set_seed(42)  # Ensure reproducibility\n",
        "    all_image_paths = tf.random.shuffle(all_image_paths)\n",
        "\n",
        "    def process_image(file_path):\n",
        "        image = tf.io.read_file(file_path)\n",
        "        image = tf.image.decode_jpeg(image, channels=3)\n",
        "        image = tf.image.resize(image, [256, 256])\n",
        "\n",
        "        image = image / 127.5 - 1\n",
        "        return image\n",
        "\n",
        "    def prepare_dataset(paths):\n",
        "        paths_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "        dataset = paths_ds.map(lambda x: process_image(x))\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    # Create train and validation datasets\n",
        "    dataset = prepare_dataset(all_image_paths)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNn7ER7l-xnq"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "anime_dir = \"/content/anime_folder/7000_anime\"\n",
        "landscape_dir = \"/content/landscape_folder/7000_landscape\"\n",
        "blurred_dir = \"/content/blurred_folder/7000_blurred\"\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "anime_dataset = create_dataset(anime_dir, batch_size=batch_size)\n",
        "landscape_dataset = create_dataset(landscape_dir, batch_size=batch_size)\n",
        "blurred_dataset = create_dataset(blurred_dir, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiRjCizM-xnq"
      },
      "outputs": [],
      "source": [
        "encoder = build_encoder()\n",
        "\n",
        "main_generator = build_main_generator(encoder=encoder)\n",
        "support_generator = build_support_generator(encoder=encoder)\n",
        "\n",
        "main_discriminator = build_main_discriminator()\n",
        "support_discriminator = build_support_discriminator()\n",
        "\n",
        "main_gan = build_main_gan(generator=main_generator, discriminator=main_discriminator)\n",
        "support_gan = build_support_gan(generator=support_generator, discriminator=support_discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "ww9zkJzkD64-",
        "outputId": "08a76db2-c9f8-4fa7-e082-a5e3a70af93e"
      },
      "outputs": [],
      "source": [
        "main_gan.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pi_HH9u-xnq"
      },
      "outputs": [],
      "source": [
        "model_vgg = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "model_vgg.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auR4RFXK-xnr",
        "outputId": "d89570da-3212-4dac-dff0-b4d2d7bae66c"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "kpKLxrpD-xnr",
        "outputId": "52b951bf-e7c8-45c7-b0e9-0841e56aeb54"
      },
      "outputs": [],
      "source": [
        "# Initialize a W&B run\n",
        "wandb.init(\n",
        "    project=\"Anime GAN v3\",  # Your project name\n",
        "    name=\"first attemts\",             # Optional: Name of the run\n",
        "    config={                         # Optional: Configurations for your run\n",
        "        \"learning_rate\": 1.0e-4,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anaw-U2N-xnr"
      },
      "outputs": [],
      "source": [
        "main_gan_optimizer = tf.keras.optimizers.Adam(learning_rate=1.0e-3)\n",
        "support_gan_optimizer = tf.keras.optimizers.Adam(learning_rate=1.0e-3)\n",
        "\n",
        "main_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
        "support_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtpGUbWSqCFB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_main_generator(original_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        main_image = main_generator(original_image)\n",
        "        support_image = support_generator(original_image)\n",
        "        revised_image = fine_grained_revision(support_image)\n",
        "\n",
        "        per_pixel_value = 2000 * per_pixel_loss(revised_image, main_image)\n",
        "        perception_value = 3 * perception_loss(revised_image, main_image, model_vgg)\n",
        "        m_adv_value = 200 * m_adv_loss(main_gan, original_image)\n",
        "        main_tv_value = 0.005 * total_loss(main_image)\n",
        "\n",
        "        main_loss = per_pixel_value + perception_value + m_adv_value + main_tv_value\n",
        "    gradients = tape.gradient(main_loss, main_gan.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 5.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    main_gan_optimizer.apply_gradients(zip(clipped_gradients, main_gan.trainable_variables))\n",
        "\n",
        "    return [main_image, per_pixel_value, perception_value, m_adv_value, main_tv_value]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "susJBnklqCFB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_support_generator(original_image, anime_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        support_image = support_generator(original_image)\n",
        "        guided_image = guided_filter_tf(support_image)\n",
        "\n",
        "\n",
        "        content_value = 2 * content_loss(original_image, guided_image, model_vgg)\n",
        "        # region_smoothing_value = region_smoothing_loss(original_image, guided_image, model_vgg)\n",
        "        region_smoothing_value = 0\n",
        "        support_tv_value = 0.004 * total_loss(guided_image)\n",
        "        color_value = 500 * color_loss(original_image, guided_image)\n",
        "        s_adv_value = 50 * s_adv_loss(support_gan, original_image)\n",
        "        gray_style_value = 0.05 * gray_style_loss(anime_image, guided_image, model_vgg)\n",
        "\n",
        "        support_loss = content_value + gray_style_value + region_smoothing_value + s_adv_value + color_value + support_tv_value\n",
        "    gradients = tape.gradient(support_loss, support_gan.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 5.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    support_gan_optimizer.apply_gradients(zip(clipped_gradients, support_gan.trainable_variables))\n",
        "\n",
        "    return [support_image, content_value, support_tv_value, color_value, s_adv_value, gray_style_value]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcD82_7cqCFB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_main_discriminator(original_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        main_image = main_generator(original_image)\n",
        "        support_image = support_generator(original_image)\n",
        "        revised_image = fine_grained_revision(support_image)\n",
        "\n",
        "        dm_value = 40 * dm_loss(main_discriminator, main_image, revised_image)\n",
        "    gradients = tape.gradient(dm_value, main_discriminator.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 3.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    main_discriminator_optimizer.apply_gradients(zip(clipped_gradients, main_discriminator.trainable_variables))\n",
        "\n",
        "    return dm_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oPb-4LbqCFB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_support_discriminator_gp(original_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        support_image = support_generator(original_image)\n",
        "        guided_image = guided_filter_tf(support_image)\n",
        "\n",
        "        ds_value = 10 * ds_loss(support_discriminator, guided_image, \"gsp\")\n",
        "    gradients = tape.gradient(ds_value, support_discriminator.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 3.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    support_discriminator_optimizer.apply_gradients(zip(clipped_gradients, support_discriminator.trainable_variables))\n",
        "    return ds_value\n",
        "\n",
        "@tf.function\n",
        "def train_support_discriminator_a(a):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ds_value = 10 * ds_loss(support_discriminator, a, \"a\")\n",
        "\n",
        "    gradients = tape.gradient(ds_value, support_discriminator.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 3.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    support_discriminator_optimizer.apply_gradients(zip(clipped_gradients, support_discriminator.trainable_variables))\n",
        "\n",
        "    return ds_value\n",
        "\n",
        "@tf.function\n",
        "def train_support_discriminator_e(e):\n",
        "    with tf.GradientTape() as tape:\n",
        "        ds_value = 10 * ds_loss(support_discriminator, e, \"e\")\n",
        "    gradients = tape.gradient(ds_value, support_discriminator.trainable_variables)\n",
        "\n",
        "    clipped_gradients = [tf.clip_by_norm(g, 3.0) for g in gradients]  # Clip gradients\n",
        "\n",
        "    support_discriminator_optimizer.apply_gradients(zip(clipped_gradients, support_discriminator.trainable_variables))\n",
        "\n",
        "    return ds_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8IVzWQUvzI4"
      },
      "outputs": [],
      "source": [
        "def denorm(image):\n",
        "    return (image + 1) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0986keo-xnr",
        "outputId": "33940500-fa54-43f6-f14b-0e76a79b356a"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    iter1 = iter(landscape_dataset)\n",
        "    iter2 = iter(anime_dataset)\n",
        "    iter3 = iter(blurred_dataset)\n",
        "    step = 0\n",
        "    while True:\n",
        "        try:\n",
        "            all_losses = {}\n",
        "\n",
        "            main_discriminator.trainable = False\n",
        "            support_discriminator.trainable = False\n",
        "\n",
        "            original_image = next(iter1)\n",
        "            anime_image = next(iter2)\n",
        "            blurred_image = next(iter3)\n",
        "\n",
        "            main_image, all_losses[\"per pixel\"], all_losses[\"perception\"], all_losses[\"m_adv\"], all_losses[\"main_tv\"] = train_main_generator(original_image)\n",
        "            support_image, all_losses[\"content value\"], all_losses[\"support tv\"], all_losses[\"color value\"], all_losses[\"s_adv\"], all_losses[\"gray style\"] = train_support_generator(original_image, anime_image)\n",
        "            if step % 1 == 0:\n",
        "                main_discriminator.trainable = True\n",
        "                support_discriminator.trainable = True\n",
        "\n",
        "                all_losses[\"dm\"] = train_main_discriminator(original_image)\n",
        "                all_losses[\"ds\"] = train_support_discriminator_gp(original_image)\n",
        "                all_losses[\"ds\"] += train_support_discriminator_a(anime_image)\n",
        "                all_losses[\"ds\"] += train_support_discriminator_e(blurred_image)\n",
        "            if step % 5 == 0:\n",
        "                wandb.log({\n",
        "                    \"main_image\": wandb.Image(denorm(main_image).numpy()[0], caption=f\"Step {step + 1}\"),\n",
        "                    \"support_image\": wandb.Image(denorm(support_image).numpy()[0], caption=f\"Step {step + 1}\"),\n",
        "                    **all_losses\n",
        "                })\n",
        "            step += 1\n",
        "        except StopIteration:\n",
        "            break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
